{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53091614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain langchain_community langgraph langchain_openai\n",
    "%pip install --quiet langchain-experimental langgraph-supervisor\n",
    "%pip install --quiet tavily-python httpx pandas tabulate matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Keys\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-dev-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Simple logging setup\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from tavily import TavilyClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba9629",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Set Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b948c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], model=\"gpt-4o-mini\", temperature=0.1)\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c7f11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Set Utils for Scratchpad Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd68b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratchpad Memory Utils\n",
    "MEMORY_FILE = \"memory_scratchpad_docs.json\"\n",
    "\n",
    "def initialize_memory_file():\n",
    "    \"\"\"Initialize the memory file if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(MEMORY_FILE):\n",
    "        initial_memory = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"last_updated\": datetime.now().isoformat(),\n",
    "            \"documentations\": {}\n",
    "        }\n",
    "        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(initial_memory, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Initialized memory file: {MEMORY_FILE}\")\n",
    "\n",
    "def load_memory() -> Dict[str, Any]:\n",
    "    \"\"\"Load the memory file\"\"\"\n",
    "    try:\n",
    "        with open(MEMORY_FILE, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        initialize_memory_file()\n",
    "        return load_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading memory: {e}\")\n",
    "        return {\"documentations\": {}}\n",
    "\n",
    "def save_memory(memory_data: Dict[str, Any]):\n",
    "    \"\"\"Save the memory file\"\"\"\n",
    "    try:\n",
    "        memory_data[\"last_updated\"] = datetime.now().isoformat()\n",
    "        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(memory_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Memory saved to {MEMORY_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving memory: {e}\")\n",
    "\n",
    "# Initialize memory file\n",
    "initialize_memory_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67c98b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Set Utils for CSV Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Memory Utils\n",
    "CSV_MEMORY_FILE = \"csv_memory.json\"\n",
    "\n",
    "def initialize_csv_memory_file():\n",
    "    \"\"\"Initialize the CSV memory file if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(CSV_MEMORY_FILE):\n",
    "        initial_csv_memory = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"last_updated\": datetime.now().isoformat(),\n",
    "            \"csv_data\": {}\n",
    "        }\n",
    "        with open(CSV_MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(initial_csv_memory, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Initialized CSV memory file: {CSV_MEMORY_FILE}\")\n",
    "\n",
    "def load_csv_memory() -> Dict[str, Any]:\n",
    "    \"\"\"Load the CSV memory file\"\"\"\n",
    "    try:\n",
    "        with open(CSV_MEMORY_FILE, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        initialize_csv_memory_file()\n",
    "        return load_csv_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV memory: {e}\")\n",
    "        return {\"csv_data\": {}}\n",
    "\n",
    "def save_csv_memory(csv_memory_data: Dict[str, Any]):\n",
    "    \"\"\"Save the CSV memory file\"\"\"\n",
    "    try:\n",
    "        csv_memory_data[\"last_updated\"] = datetime.now().isoformat()\n",
    "        with open(CSV_MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(csv_memory_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"CSV memory saved to {CSV_MEMORY_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV memory: {e}\")\n",
    "\n",
    "def store_csv_data(csv_name: str, csv_content: str, source: str = \"OpenF1\"):\n",
    "    \"\"\"Store CSV data in persistent file\"\"\"\n",
    "    csv_memory = load_csv_memory()\n",
    "    csv_memory[\"csv_data\"][csv_name] = {\n",
    "        \"content\": csv_content,\n",
    "        \"source\": source,\n",
    "        \"stored_at\": datetime.now().isoformat(),\n",
    "        \"size\": len(csv_content)\n",
    "    }\n",
    "    save_csv_memory(csv_memory)\n",
    "    print(f\"CSV data stored: {csv_name} ({len(csv_content)} characters)\")\n",
    "\n",
    "def get_csv_data(csv_name: str) -> str:\n",
    "    \"\"\"Get CSV data from persistent file\"\"\"\n",
    "    csv_memory = load_csv_memory()\n",
    "    if csv_name in csv_memory.get(\"csv_data\", {}):\n",
    "        return csv_memory[\"csv_data\"][csv_name][\"content\"]\n",
    "    return None\n",
    "\n",
    "def list_available_csvs() -> Dict[str, Any]:\n",
    "    \"\"\"List all available CSV datasets in persistent storage\"\"\"\n",
    "    csv_memory = load_csv_memory()\n",
    "    csv_data = csv_memory.get(\"csv_data\", {})\n",
    "    \n",
    "    if not csv_data:\n",
    "        return {\"message\": \"No CSV datasets available\"}\n",
    "    \n",
    "    result = {\"available_datasets\": {}}\n",
    "    for name, data in csv_data.items():\n",
    "        result[\"available_datasets\"][name] = {\n",
    "            \"source\": data[\"source\"],\n",
    "            \"stored_at\": data[\"stored_at\"],\n",
    "            \"size\": data[\"size\"]\n",
    "        }\n",
    "    return result\n",
    "\n",
    "# Helper function for loading DataFrames\n",
    "def load_dataframe_from_csv(csv_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Load DataFrame from CSV data stored in persistent file\"\"\"\n",
    "    # Get CSV content from persistent storage\n",
    "    csv_content = get_csv_data(csv_name)\n",
    "    if csv_content is None:\n",
    "        raise ValueError(f\"CSV '{csv_name}' not found in persistent storage\")\n",
    "    \n",
    "    # Load DataFrame from CSV content\n",
    "    from io import StringIO\n",
    "    df = pd.read_csv(StringIO(csv_content))\n",
    "    \n",
    "    print(f\"DataFrame loaded: {csv_name} ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "    return df\n",
    "\n",
    "# Initialize CSV memory file\n",
    "initialize_csv_memory_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b40af2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools for Context Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94791263",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_documentation_from_website(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract documentation content from a given website URL.\n",
    "    This tool extracts raw documentation text from a website, \n",
    "    which can later be processed or analyzed to identify API endpoints, \n",
    "    parameters, and other technical details.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content_response = tavily_client.extract(urls=url)\n",
    "\n",
    "        if content_response and len(content_response) > 0:\n",
    "            return f\"Documentation extracted from {url}:\\n\\n{content_response}\"\n",
    "       \n",
    "        else:\n",
    "            return f\"Could not extract content from {url}. Please check the URL and try again.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error extracting documentation from {url}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Debug Tools\n",
    "@tool\n",
    "def debug_csv_storage() -> str:\n",
    "    \"\"\"\n",
    "    Debug tool to check the current state of CSV storage.\n",
    "    This helps diagnose issues with data sharing between agents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = \"=== CSV STORAGE DEBUG ===\\n\\n\"\n",
    "        \n",
    "        # Check persistent storage\n",
    "        csv_memory = load_csv_memory()\n",
    "        csv_data = csv_memory.get(\"csv_data\", {})\n",
    "        result += f\"Persistent CSV storage: {len(csv_data)} items\\n\"\n",
    "        for name, data in csv_data.items():\n",
    "            result += f\"  - {name}: {data['size']} chars, source: {data['source']}\\n\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error in debug_csv_storage: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def list_available_data() -> str:\n",
    "    \"\"\"\n",
    "    List all available data sources for analysis.\n",
    "    This provides a comprehensive view of what data is available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = \"=== AVAILABLE DATA SOURCES ===\\n\\n\"\n",
    "        \n",
    "        # Persistent Storage\n",
    "        result += \"CSV Storage:\\n\"\n",
    "        csv_memory = load_csv_memory()\n",
    "        csv_data = csv_memory.get(\"csv_data\", {})\n",
    "        if csv_data:\n",
    "            for name, data in csv_data.items():\n",
    "                result += f\"   - {name}: {data['size']} chars, source: {data['source']}\\n\"\n",
    "        else:\n",
    "            result += \"   - No CSV data available\\n\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error listing available data: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_documentation_summary(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a concise summary of documentation content in Markdown format.\n",
    "    This tool processes raw documentation text, removes irrelevant details, \n",
    "    and creates a well-formatted Markdown file with API endpoints and parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt_summary_agent = \"\"\"\n",
    "    You are an expert in summarizing API documentation.\n",
    "    You are given raw documentation text and must extract only the relevant information \n",
    "    about API endpoints and their parameters. Create a well-formatted Markdown summary.\n",
    "    \n",
    "    Format the output as a Markdown document with:\n",
    "    - Clear headings for each endpoint\n",
    "    - HTTP method and URL\n",
    "    - Parameter descriptions in tables\n",
    "    - Code examples when available\n",
    "    - Keep it concise but informative\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_summary_agent),\n",
    "        (\"user\", \"Here is the content to be summarized: {content}\")\n",
    "    ])\n",
    "    \n",
    "    response = llm.invoke(prompt.format(content=content))\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_api_data(endpoint: str, parameters: Dict[str, Any] = None) -> str:\n",
    "    \"\"\"\n",
    "    Fetch data from a specified API endpoint.\n",
    "    This tool constructs the request URL with optional parameters, \n",
    "    performs an HTTP GET request, and returns the response content.\n",
    "    For OpenF1 API endpoints, automatically requests CSV format and stores in memory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if this is an OpenF1 API endpoint\n",
    "        is_openf1 = \"api.openf1.org\" in endpoint\n",
    "        \n",
    "        if parameters:\n",
    "            # Convert parameters to query string\n",
    "            param_strings = []\n",
    "            for key, value in parameters.items():\n",
    "                param_strings.append(f\"{key}={value}\")\n",
    "            if param_strings:\n",
    "                endpoint += \"?\" + \"&\".join(param_strings)\n",
    "        \n",
    "        # For OpenF1 API, automatically append csv=true parameter (only if not already present)\n",
    "        if is_openf1 and \"csv=true\" not in endpoint:\n",
    "            separator = \"&\" if \"?\" in endpoint else \"?\"\n",
    "            endpoint += f\"{separator}csv=true\"\n",
    "        \n",
    "        # Make the HTTP request\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get(endpoint, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # For OpenF1 CSV responses, store in memory and return confirmation\n",
    "            if is_openf1 and response.headers.get('content-type', '').startswith('text/csv'):\n",
    "                # Generate CSV name based on endpoint (including all filters)\n",
    "                csv_name = generate_csv_name(endpoint, parameters)\n",
    "                store_csv_data(csv_name, response.text, \"OpenF1\")\n",
    "                return f\"CSV data fetched and stored as '{csv_name}' from {endpoint}\\n\\nData preview (first 5 lines):\\n{response.text.split(chr(10))[:5]}\"\n",
    "            \n",
    "            # For other APIs, try JSON first, then fall back to text\n",
    "            try:\n",
    "                data = response.json()\n",
    "                return f\"API Response from {endpoint}:\\n\\n{json.dumps(data, indent=2)}\"\n",
    "            except json.JSONDecodeError:\n",
    "                return f\"API Response from {endpoint}:\\n\\n{response.text}\"\n",
    "                \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        return f\"HTTP Error {e.response.status_code} when fetching {endpoint}: {e.response.text}\"\n",
    "    except httpx.TimeoutException:\n",
    "        return f\"Timeout error when fetching {endpoint}. The request took too long.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching data from {endpoint}: {str(e)}\"\n",
    "\n",
    "def generate_csv_name(endpoint: str, parameters: Dict[str, Any] = None) -> str:\n",
    "    \"\"\"Generate a CSV name based on endpoint and all parameters (including URL filters)\"\"\"\n",
    "    # Extract endpoint type (e.g., 'laps', 'sessions', 'drivers')\n",
    "    endpoint_clean = endpoint.split('?')[0].split('/')[-1] if endpoint else \"data\"\n",
    "    \n",
    "    # Extract all parameters from URL and combine with passed parameters\n",
    "    all_params = {}\n",
    "    \n",
    "    # Parse URL parameters\n",
    "    if '?' in endpoint:\n",
    "        url_params = endpoint.split('?')[1]\n",
    "        for param in url_params.split('&'):\n",
    "            if '=' in param:\n",
    "                key, value = param.split('=', 1)\n",
    "                if key != 'csv':  # Skip csv parameter\n",
    "                    all_params[key] = value\n",
    "    \n",
    "    # Add passed parameters (overriding URL params if same key)\n",
    "    if parameters:\n",
    "        for key, value in parameters.items():\n",
    "            if key != 'csv':  # Skip csv parameter\n",
    "                all_params[key] = value\n",
    "    \n",
    "    # Create suffix from all parameters\n",
    "    param_suffix = \"\"\n",
    "    if all_params:\n",
    "        param_parts = []\n",
    "        for key, value in sorted(all_params.items()):  # Sort for consistency\n",
    "            # Clean parameter values for filename\n",
    "            clean_value = str(value).replace('=', '').replace('&', '').replace('?', '').replace('<', 'lt').replace('>', 'gt')\n",
    "            param_parts.append(f\"{key}_{clean_value}\")\n",
    "        if param_parts:\n",
    "            param_suffix = \"_\" + \"_\".join(param_parts)\n",
    "    \n",
    "    return f\"openf1_{endpoint_clean}{param_suffix}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def look_memory(website_url: str = None, api_name: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Look up stored API documentation summaries in the memory scratchpad.\n",
    "\n",
    "    This tool checks if documentation for a given website or API is already \n",
    "    available in memory, avoiding the need to re-extract and re-summarize. \n",
    "    It can return either specific matches or an overview of all stored summaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        memory = load_memory()\n",
    "        documentations = memory.get(\"documentations\", {})\n",
    "        \n",
    "        if not documentations:\n",
    "            return \"Memory scratchpad is empty. No API documentation summaries found.\"\n",
    "        \n",
    "        # If specific URL or API name provided, search for it\n",
    "        if website_url or api_name:\n",
    "            found_docs = []\n",
    "            for doc_id, doc_data in documentations.items():\n",
    "                doc_url = doc_data.get(\"source_url\", \"\").lower()\n",
    "                doc_name = doc_data.get(\"api_name\", \"\").lower()\n",
    "                \n",
    "                if (website_url and website_url.lower() in doc_url) or \\\n",
    "                   (api_name and api_name.lower() in doc_name):\n",
    "                    found_docs.append({\n",
    "                        \"id\": doc_id,\n",
    "                        \"api_name\": doc_data.get(\"api_name\", \"Unknown\"),\n",
    "                        \"source_url\": doc_data.get(\"source_url\", \"Unknown\"),\n",
    "                        \"created_at\": doc_data.get(\"created_at\", \"Unknown\"),\n",
    "                        \"summary\": doc_data.get(\"summary\", \"No summary available\")\n",
    "                    })\n",
    "            \n",
    "            if found_docs:\n",
    "                result = f\"Found {len(found_docs)} documentation summary(ies) in memory:\\n\\n\"\n",
    "                for doc in found_docs:\n",
    "                    result += f\"**{doc['api_name']}**\\n\"\n",
    "                    result += f\"   URL: {doc['source_url']}\\n\"\n",
    "                    result += f\"   Created: {doc['created_at']}\\n\"\n",
    "                    result += f\"   Summary: {doc['summary'][:200]}...\\n\\n\"\n",
    "                return result\n",
    "            else:\n",
    "                return f\"No documentation found for {'website: ' + website_url if website_url else ''}{'API: ' + api_name if api_name else ''}\"\n",
    "        \n",
    "        # If no specific search, return overview of all documentation\n",
    "        result = f\"Memory scratchpad contains {len(documentations)} API documentation summary(ies):\\n\\n\"\n",
    "        for doc_id, doc_data in documentations.items():\n",
    "            result += f\"**{doc_data.get('api_name', 'Unknown API')}**\\n\"\n",
    "            result += f\"   URL: {doc_data.get('source_url', 'Unknown')}\\n\"\n",
    "            result += f\"   Created: {doc_data.get('created_at', 'Unknown')}\\n\\n\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error looking up memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def write_memory(api_name: str, source_url: str, summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Save a new API documentation summary into the memory scratchpad.\n",
    "    This tool stores structured documentation so it can be reused later \n",
    "    without re-extracting and re-summarizing the same API source.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        memory = load_memory()\n",
    "        \n",
    "        # Create a unique ID for this documentation\n",
    "        doc_id = f\"{api_name.lower().replace(' ', '_')}_{int(datetime.now().timestamp())}\"\n",
    "        \n",
    "        # Create the documentation entry\n",
    "        doc_entry = {\n",
    "            \"api_name\": api_name,\n",
    "            \"source_url\": source_url,\n",
    "            \"summary\": summary,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"endpoints_count\": len(json.loads(summary).get(\"endpoints\", [])) if summary.startswith('{') else 0\n",
    "        }\n",
    "        \n",
    "        # Add to memory\n",
    "        memory[\"documentations\"][doc_id] = doc_entry\n",
    "        \n",
    "        # Save memory\n",
    "        save_memory(memory)\n",
    "        \n",
    "        return f\"Successfully saved documentation for **{api_name}** to memory scratchpad!\\n\\n\" \\\n",
    "               f\"**Details:**\\n\" \\\n",
    "               f\"   API: {api_name}\\n\" \\\n",
    "               f\"   Source: {source_url}\\n\" \\\n",
    "               f\"   Endpoints: {doc_entry['endpoints_count']}\\n\" \\\n",
    "               f\"   Saved at: {doc_entry['created_at']}\\n\\n\" \\\n",
    "               f\"This documentation can now be reused for future API calls without re-extraction.\"\n",
    "               \n",
    "    except Exception as e:\n",
    "        return f\"Error writing to memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e432299",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools for Analysis Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d954331",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_memory_documentation(api_name: str = None, website_url: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve a stored API documentation summary from the memory scratchpad.\n",
    "    This tool returns the full documentation entry for a given API name \n",
    "    or website URL, allowing reuse without re-extracting the source.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        memory = load_memory()\n",
    "        documentations = memory.get(\"documentations\", {})\n",
    "        \n",
    "        if not documentations:\n",
    "            return \"No documentation found in memory.\"\n",
    "        \n",
    "        # Find matching documentation\n",
    "        for doc_id, doc_data in documentations.items():\n",
    "            doc_url = doc_data.get(\"source_url\", \"\").lower()\n",
    "            doc_name = doc_data.get(\"api_name\", \"\").lower()\n",
    "            \n",
    "            if (api_name and api_name.lower() in doc_name) or \\\n",
    "               (website_url and website_url.lower() in doc_url):\n",
    "                \n",
    "                return f\"**Retrieved from Memory:** {doc_data['api_name']}\\n\" \\\n",
    "                       f\"Source: {doc_data['source_url']}\\n\" \\\n",
    "                       f\"Created: {doc_data['created_at']}\\n\\n\" \\\n",
    "                       f\"**Documentation Summary:**\\n{doc_data['summary']}\"\n",
    "        \n",
    "        return f\"No documentation found for {'API: ' + api_name if api_name else ''}{'Website: ' + website_url if website_url else ''}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving documentation from memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def analyze_data_with_pandas(analysis_query: str, csv_names: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Analyze CSV datasets using a Pandas DataFrame agent.\n",
    "    This tool loads CSV data from persistent storage and allows natural language queries \n",
    "    on the datasets by leveraging a Pandas agent that executes LLM-generated Python code.\n",
    "    It can work with multiple DataFrames simultaneously for comparative analysis.\n",
    "    \n",
    "    Args:\n",
    "        analysis_query: The analysis query in natural language\n",
    "        csv_names: Comma-separated list of CSV names to analyze. If None, analyzes all available CSVs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get list of available CSVs\n",
    "        available_csvs = list_available_csvs()\n",
    "        \n",
    "        if \"message\" in available_csvs:\n",
    "            return \"No CSV datasets available. Please fetch some data from OpenF1 API first.\"\n",
    "        \n",
    "        # Get available CSV names\n",
    "        available_names = list(available_csvs[\"available_datasets\"].keys())\n",
    "        \n",
    "        # Determine which CSVs to analyze\n",
    "        if csv_names:\n",
    "            csv_list = [name.strip() for name in csv_names.split(',')]\n",
    "            # Filter to only include available CSVs\n",
    "            csv_list = [name for name in csv_list if name in available_names]\n",
    "        else:\n",
    "            csv_list = available_names\n",
    "        \n",
    "        if not csv_list:\n",
    "            return \"No valid CSV names provided or no CSVs available.\"\n",
    "        \n",
    "        # Load DataFrames directly from persistent storage\n",
    "        dataframes_list = []\n",
    "        dataframe_names = []\n",
    "        \n",
    "        for csv_name in csv_list:\n",
    "            try:\n",
    "                # Load DataFrame using helper function\n",
    "                df = load_dataframe_from_csv(csv_name)\n",
    "                \n",
    "                # Create a clean name for the DataFrame\n",
    "                clean_name = csv_name.replace('.csv', '').replace('openf1_', '')\n",
    "                dataframes_list.append(df)\n",
    "                dataframe_names.append(f\"df_{clean_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {csv_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not dataframes_list:\n",
    "            return \"No DataFrames could be loaded successfully.\"\n",
    "        \n",
    "        # Create pandas agent with all dataframes\n",
    "        agent = create_pandas_dataframe_agent(\n",
    "            ChatOpenAI(temperature=0, model=\"gpt-4o-mini\"),\n",
    "            dataframes_list,\n",
    "            verbose=True,\n",
    "            agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "            allow_dangerous_code=True\n",
    "        )\n",
    "        \n",
    "        result = agent.invoke(analysis_query)\n",
    "        return f\"Analysis of {len(dataframes_list)} CSV datasets:\\n\\n{result}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Analysis error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a50dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools Bindings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools Bindings for Context Agent\n",
    "tools_context_agent = [\n",
    "    extract_documentation_from_website, \n",
    "    create_documentation_summary,\n",
    "    fetch_api_data,\n",
    "    look_memory,\n",
    "    write_memory,\n",
    "    get_memory_documentation,\n",
    "]\n",
    "\n",
    "print(\"Available tools:\")\n",
    "for i, tool in enumerate(tools_context_agent, 1):\n",
    "    print(f\"   {i}. {tool.name}: {tool.description.split('.')[0] if tool.description else 'No description'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools Bindings for Analysis Agent\n",
    "tools_analysis_agent = [\n",
    "    analyze_data_with_pandas,\n",
    "    debug_csv_storage,\n",
    "    list_available_data\n",
    "]\n",
    "\n",
    "print(\"Available tools:\")\n",
    "for i, tool in enumerate(tools_analysis_agent, 1):\n",
    "    print(f\"   {i}. {tool.name}: {tool.description.split('.')[0] if tool.description else 'No description'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4706d91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Systems Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt Context Agent\n",
    "system_prompt_context_agent = \"\"\"\n",
    "You are a specialized API assistant with long-term memory capabilities that helps users learn and interact with APIs efficiently.\n",
    "\n",
    "## Your Memory System:\n",
    "You have access to a persistent memory scratchpad that stores API documentation summaries. This allows you to:\n",
    "- Avoid re-extracting documentation you've already processed\n",
    "- Provide faster responses by using cached knowledge\n",
    "- Build up a knowledge base of API documentation over time\n",
    "\n",
    "## Your Workflow:\n",
    "1. **ALWAYS start by checking memory** using `look_memory` to see if you already have documentation for the requested API\n",
    "2. **If documentation exists in memory**: Use `get_memory_documentation` to retrieve it and proceed directly to API calls\n",
    "3. **If no documentation in memory**: \n",
    "   - Extract documentation using `extract_documentation_from_website`\n",
    "   - Summarize it using `create_documentation_summary` \n",
    "   - Save it to memory using `write_memory`\n",
    "\n",
    "## Available Tools and How to Use Them:\n",
    "\n",
    "### 1. Memory Management Tools:\n",
    "- `look_memory(website_url=\"https://example.com\")` - Check if documentation exists for a specific URL\n",
    "- `look_memory(api_name=\"OpenF1\")` - Check if documentation exists for a specific API name\n",
    "- `get_memory_documentation(api_name=\"OpenF1\")` - Retrieve full documentation from memory\n",
    "- `write_memory(api_name=\"OpenF1\", source_url=\"https://openf1.org\", summary=\"...\")` - Save new documentation\n",
    "\n",
    "### 2. Documentation Tools:\n",
    "- `extract_documentation_from_website(url=\"https://example.com\")` - Extract raw documentation from a website\n",
    "- `create_documentation_summary(content=\"...\")` - Convert raw documentation into structured summary\n",
    "\n",
    "### 3. API Data Fetching Tool:\n",
    "- `fetch_api_data(endpoint=\"https://api.example.com/v1/data\")` - Fetch data from any API endpoint\n",
    "- `fetch_api_data(endpoint=\"https://api.example.com/v1/data\", parameters={\"foo\": xxx, \"bar\": \"yyy\"})` - Fetch with parameters\n",
    "\n",
    "**IMPORTANT**: For OpenF1 API, the system automatically adds `csv=true` parameter to get CSV format. You can still add other parameters:\n",
    "- Example: `fetch_api_data(endpoint=\"https://api.openf1.org/v1/laps\", parameters={\"example_parameter_1\": example_value_1, \"example_parameter_2\": example_value_2})`\n",
    "- This will become: `https://api.openf1.org/v1/laps?example_parameter_1=example_value_1&example_parameter_2=example_value_2&csv=true`\n",
    "\n",
    "## Key Behaviors:\n",
    "- **Efficiency First**: Always check memory before extracting new documentation\n",
    "- **Memory Building**: Save all new documentation summaries to build your knowledge base\n",
    "- **Context Awareness**: Use the retrieved summary documentation to provide information on how to use the API\n",
    "- **User Communication**: Always explain what you're doing and whether you're using cached or new information\n",
    "- **Rate Limit Protection**: Minimize API calls by fetching comprehensive datasets in single calls\n",
    "- **Tool Usage**: You can call the tools as many times as you want to get the information you need\n",
    "\n",
    "## CRITICAL: API Rate Limit Rules:\n",
    "- **AVOID making multiple API calls** when one comprehensive call would work\n",
    "- **ALWAYS fetch the largest dataset possible** in a single call\n",
    "- **Only apply specific filters** when user explicitly requests them\n",
    "\n",
    "## For OpenF1 API specifically:\n",
    "- Check memory for \"OpenF1\" or \"openf1.org\" documentation first\n",
    "- If not found, extract from https://openf1.org/#api-endpoints\n",
    "- Save the structured summary to memory for future use\n",
    "\n",
    "## Data Fetching Strategy - CRITICAL RULES:\n",
    "\n",
    "### API RATE LIMIT PROTECTION:\n",
    "- **MAXIMIZE DATA PER CALL**: Always fetch the largest possible dataset in a single API call\n",
    "- **PREFER COMPLETE DATASETS**: When in doubt, fetch complete datasets without filters to avoid data loss\n",
    "- **USE FILTERS ONLY WHEN EXPLICITLY REQUESTED**: Only apply filters when the user specifically requests filtered data\n",
    "\n",
    "### Fetching Examples:\n",
    "- **GOOD**: `fetch_api_data(endpoint=\"https://api.openf1.org/v1/laps\")` - Gets ALL laps\n",
    "- **GOOD**: `fetch_api_data(endpoint=\"https://api.openf1.org/v1/sessions\")` - Gets ALL sessions\n",
    "- **BAD**: Multiple calls like `fetch_api_data(..., parameters={\"driver_number\": 1})` then `fetch_api_data(..., parameters={\"driver_number\": 2})`\n",
    "- **BAD**: Fetching small filtered datasets when user asks for \"performance analysis\"\n",
    "\n",
    "### Smart Fetching Strategy:\n",
    "- **For general analysis requests**: Fetch complete datasets without filters\n",
    "- **For specific requests**: Only then apply the specific filter if it doesn't limit the analysis capabilities\n",
    "\n",
    "Be helpful, efficient, and always leverage your memory system to provide the best experience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263aa1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt Analysis Agent\n",
    "system_prompt_analysis_agent = \"\"\"\n",
    "You are a specialized data analysis agent that can analyze CSV datasets using pandas and generate visualizations.\n",
    "\n",
    "## Your Context:\n",
    "You can analyze CSV data that was fetched by the Context Agent from APIs (like OpenF1). This data is stored persistently and can be accessed directly when needed. The system supports multiple DataFrames simultaneously, allowing for comparative analysis across different CSV files.\n",
    "\n",
    "## Available Tools and How to Use Them:\n",
    "\n",
    "### 1. Data Discovery Tools:\n",
    "- `list_available_data()` - Get a comprehensive view of all available data sources\n",
    "- `debug_csv_storage()` - Check what CSV data is available in persistent storage (more detailed)\n",
    "\n",
    "### 2. Analysis Tool:\n",
    "- `analyze_data_with_pandas(analysis_query=\"your question here\")` - Analyze all available CSV datasets\n",
    "- `analyze_data_with_pandas(analysis_query=\"your question here\", csv_names=\"dataset1,dataset2\")` - Analyze specific CSV datasets\n",
    "\n",
    "**IMPORTANT**: The analysis tool can:\n",
    "- Work with multiple DataFrames simultaneously\n",
    "- Perform joins and merges between DataFrames\n",
    "- Generate visualizations (graphs, charts, plots)\n",
    "- Execute complex pandas operations\n",
    "- Answer natural language questions about the data\n",
    "\n",
    "## Your Workflow - MANDATORY STEPS:\n",
    "1. **ALWAYS start by checking what data is available** using `list_available_data()` or `debug_csv_storage()`\n",
    "2. **If no data is available**: Ask the user to fetch data using the Context Agent first\n",
    "3. **BEFORE EVERY ANALYSIS**: Always call `list_available_data()` to get the most current list of datasets\n",
    "4. **Once data is available**: Use `analyze_data_with_pandas()` to perform analysis\n",
    "\n",
    "## CRITICAL: Always Check for New Data:\n",
    "- **NEVER assume** you know what datasets are available\n",
    "- **ALWAYS call** `list_available_data()` before any analysis\n",
    "- **New datasets** may have been added by the Context Agent since your last check\n",
    "- **This prevents** analyzing outdated or incomplete data\n",
    "\n",
    "## Analysis Capabilities:\n",
    "- **Multi-Dataset Analysis**: Compare data across different CSV files, perform joins, find relationships\n",
    "- **Visualization**: Generate graphs, charts, plots to help users understand the data\n",
    "- **Complex Queries**: Answer natural language questions about the data\n",
    "\n",
    "## Key Behaviors:\n",
    "- **Always check data availability first** before attempting analysis\n",
    "- **MANDATORY: Call `list_available_data()` before every analysis** to get current datasets\n",
    "- **Use natural language** for your analysis queries - the tool understands complex questions\n",
    "- **Generate visualizations** when they help explain the data\n",
    "- **Be specific** about which datasets to analyze when needed\n",
    "- **You can call tools multiple times** to get the information you need\n",
    "\n",
    "Be helpful, efficient, and always provide clear insights with visualizations when appropriate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99a519",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **ReAct Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6880a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Context Agent\n",
    "context_agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools_context_agent, \n",
    "    prompt=system_prompt_context_agent,\n",
    "    checkpointer=InMemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235db684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Analysis Agent\n",
    "analysis_agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools_analysis_agent, \n",
    "    prompt=system_prompt_analysis_agent,\n",
    "    checkpointer=InMemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3155d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Testing Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8406255",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"test_multi_df_session\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = context_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What documentation I have in memory?\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = context_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Get info and save at memory about the OpenF1 API from https://openf1.org/\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = context_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"How I can use the OpenF1 API to get the lap data?\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed33902",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = context_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"How I can use the OpenF1 API to get the positions of a driver during a race?\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = analysis_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Which datasets I have in memory?\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
