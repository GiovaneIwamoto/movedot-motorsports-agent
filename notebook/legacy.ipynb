{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53091614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain langchain_community langgraph langchain_openai\n",
    "%pip install --quiet langchain-experimental langgraph-supervisor\n",
    "%pip install --quiet tavily-python httpx pandas kagglehub tabulate matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c35564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Keys\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-dev\"\n",
    "\n",
    "CURRENT_DATASET = None\n",
    "CURRENT_DATASET_NAME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2aa38fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovaneiwamoto/Documents/Repositories/movedot-motorsports-agent/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "from tavily import TavilyClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba9629",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Set Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b948c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], model=\"gpt-4o-mini\", temperature=0.1)\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c7f11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Set Utils for Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd68b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized memory file: memory_scratchpad_docs.json\n"
     ]
    }
   ],
   "source": [
    "# Memory Utils\n",
    "MEMORY_FILE = \"memory_scratchpad_docs.json\"\n",
    "\n",
    "def initialize_memory_file():\n",
    "    \"\"\"Initialize the memory file if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(MEMORY_FILE):\n",
    "        initial_memory = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"last_updated\": datetime.now().isoformat(),\n",
    "            \"documentations\": {}\n",
    "        }\n",
    "        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(initial_memory, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Initialized memory file: {MEMORY_FILE}\")\n",
    "\n",
    "def load_memory() -> Dict[str, Any]:\n",
    "    \"\"\"Load the memory file\"\"\"\n",
    "    try:\n",
    "        with open(MEMORY_FILE, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        initialize_memory_file()\n",
    "        return load_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading memory: {e}\")\n",
    "        return {\"documentations\": {}}\n",
    "\n",
    "def save_memory(memory_data: Dict[str, Any]):\n",
    "    \"\"\"Save the memory file\"\"\"\n",
    "    try:\n",
    "        memory_data[\"last_updated\"] = datetime.now().isoformat()\n",
    "        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(memory_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Memory saved to {MEMORY_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving memory: {e}\")\n",
    "\n",
    "# Initialize memory file\n",
    "initialize_memory_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1b219",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools for Context Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94791263",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_documentation_from_website(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract documentation content from a given website URL.\n",
    "    This tool extracts raw documentation text from a website, \n",
    "    which can later be processed or analyzed to identify API endpoints, \n",
    "    parameters, and other technical details.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content_response = tavily_client.extract(urls=url)\n",
    "\n",
    "        if content_response and len(content_response) > 0:\n",
    "            return f\"Documentation extracted from {url}:\\n\\n{content_response}\"\n",
    "       \n",
    "        else:\n",
    "            return f\"Could not extract content from {url}. Please check the URL and try again.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error extracting documentation from {url}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dca2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: This tool is not parsing as expected, it's not returning a structured JSON schema.\n",
    "\n",
    "@tool\n",
    "def create_documentation_summary(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a concise summary of documentation content.\n",
    "    This tool processes raw documentation text, removes irrelevant details, \n",
    "    and keeps only the essential information. The summary highlights API \n",
    "    endpoints and parameter usage to make the documentation easier to analyze.\n",
    "    \"\"\"\n",
    "    \n",
    "    from pydantic import BaseModel\n",
    "    from typing import List, Optional\n",
    "\n",
    "    class Attribute(BaseModel):\n",
    "        name: str\n",
    "        description: str\n",
    "\n",
    "    class Endpoint(BaseModel):\n",
    "        name: str\n",
    "        description: str\n",
    "        http_request: str\n",
    "        url: str\n",
    "        attributes: List[Attribute]\n",
    "\n",
    "    class DocumentationSummary(BaseModel):\n",
    "        endpoints: List[Endpoint]\n",
    "\n",
    "\n",
    "    system_prompt_summary_agent = f\"\"\"\n",
    "    <CONTEXT>\n",
    "    You are an expert in summarizing API documentation.\n",
    "    You are given raw documentation text and must extract only the relevant information \n",
    "    about API endpoints and their parameters. The purpose of this summary is to make it \n",
    "    easier for another agent to learn how to call the API.\n",
    "    </CONTEXT>\n",
    "\n",
    "    <INSTRUCTIONS>\n",
    "    - Return the output strictly in the provided JSON schema format. \n",
    "    - Do not include any explanatory text, notes, or additional keys.\n",
    "    - Use only the information explicitly present in the input content. \n",
    "    - Be as specific and detailed as possible with names and descriptions.\n",
    "\n",
    "    Never infer or create endpoints, parameters, or attributes that are not mentioned.\n",
    "    </INSTRUCTIONS>\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt_summary_agent),\n",
    "        (\"user\", \"Here is the content to be summarized: {content}\")\n",
    "    ])\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(DocumentationSummary)\n",
    "    response = structured_llm.invoke({\"content\": content})\n",
    "\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29cc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_api_data(endpoint: str, parameters: Dict[str, Any] = None) -> str:\n",
    "    \"\"\"\n",
    "    Fetch data from a specified API endpoint.\n",
    "    This tool constructs the request URL with optional parameters, \n",
    "    performs an HTTP GET request, and returns the response content. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        if parameters:\n",
    "            # Convert parameters to query string\n",
    "            param_strings = []\n",
    "            for key, value in parameters.items():\n",
    "                param_strings.append(f\"{key}={value}\")\n",
    "            if param_strings:\n",
    "                endpoint += \"?\" + \"&\".join(param_strings)\n",
    "        \n",
    "        # Make the HTTP request\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get(endpoint, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            try:\n",
    "                data = response.json()\n",
    "                return f\"API Response from {endpoint}:\\n\\n{json.dumps(data, indent=2)}\"\n",
    "            except json.JSONDecodeError:\n",
    "                return f\"API Response from {endpoint}:\\n\\n{response.text}\"\n",
    "                \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        return f\"HTTP Error {e.response.status_code} when fetching {endpoint}: {e.response.text}\"\n",
    "    except httpx.TimeoutException:\n",
    "        return f\"Timeout error when fetching {endpoint}. The request took too long.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching data from {endpoint}: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1903f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def look_memory(website_url: str = None, api_name: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Look up stored API documentation summaries in the memory scratchpad.\n",
    "\n",
    "    This tool checks if documentation for a given website or API is already \n",
    "    available in memory, avoiding the need to re-extract and re-summarize. \n",
    "    It can return either specific matches or an overview of all stored summaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        memory = load_memory()\n",
    "        documentations = memory.get(\"documentations\", {})\n",
    "        \n",
    "        if not documentations:\n",
    "            return \"Memory scratchpad is empty. No API documentation summaries found.\"\n",
    "        \n",
    "        # If specific URL or API name provided, search for it\n",
    "        if website_url or api_name:\n",
    "            found_docs = []\n",
    "            for doc_id, doc_data in documentations.items():\n",
    "                doc_url = doc_data.get(\"source_url\", \"\").lower()\n",
    "                doc_name = doc_data.get(\"api_name\", \"\").lower()\n",
    "                \n",
    "                if (website_url and website_url.lower() in doc_url) or \\\n",
    "                   (api_name and api_name.lower() in doc_name):\n",
    "                    found_docs.append({\n",
    "                        \"id\": doc_id,\n",
    "                        \"api_name\": doc_data.get(\"api_name\", \"Unknown\"),\n",
    "                        \"source_url\": doc_data.get(\"source_url\", \"Unknown\"),\n",
    "                        \"created_at\": doc_data.get(\"created_at\", \"Unknown\"),\n",
    "                        \"summary\": doc_data.get(\"summary\", \"No summary available\")\n",
    "                    })\n",
    "            \n",
    "            if found_docs:\n",
    "                result = f\"Found {len(found_docs)} documentation summary(ies) in memory:\\n\\n\"\n",
    "                for doc in found_docs:\n",
    "                    result += f\"**{doc['api_name']}**\\n\"\n",
    "                    result += f\"   URL: {doc['source_url']}\\n\"\n",
    "                    result += f\"   Created: {doc['created_at']}\\n\"\n",
    "                    result += f\"   Summary: {doc['summary'][:200]}...\\n\\n\"\n",
    "                return result\n",
    "            else:\n",
    "                return f\"No documentation found for {'website: ' + website_url if website_url else ''}{'API: ' + api_name if api_name else ''}\"\n",
    "        \n",
    "        # If no specific search, return overview of all documentation\n",
    "        result = f\"Memory scratchpad contains {len(documentations)} API documentation summary(ies):\\n\\n\"\n",
    "        for doc_id, doc_data in documentations.items():\n",
    "            result += f\"**{doc_data.get('api_name', 'Unknown API')}**\\n\"\n",
    "            result += f\"   URL: {doc_data.get('source_url', 'Unknown')}\\n\"\n",
    "            result += f\"   Created: {doc_data.get('created_at', 'Unknown')}\\n\\n\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error looking up memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706f917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def write_memory(api_name: str, source_url: str, summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Save a new API documentation summary into the memory scratchpad.\n",
    "    This tool stores structured documentation so it can be reused later \n",
    "    without re-extracting and re-summarizing the same API source.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        memory = load_memory()\n",
    "        \n",
    "        # Create a unique ID for this documentation\n",
    "        doc_id = f\"{api_name.lower().replace(' ', '_')}_{int(datetime.now().timestamp())}\"\n",
    "        \n",
    "        # Create the documentation entry\n",
    "        doc_entry = {\n",
    "            \"api_name\": api_name,\n",
    "            \"source_url\": source_url,\n",
    "            \"summary\": summary,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"endpoints_count\": len(json.loads(summary).get(\"endpoints\", [])) if summary.startswith('{') else 0\n",
    "        }\n",
    "        \n",
    "        # Add to memory\n",
    "        memory[\"documentations\"][doc_id] = doc_entry\n",
    "        \n",
    "        # Save memory\n",
    "        save_memory(memory)\n",
    "        \n",
    "        return f\"Successfully saved documentation for **{api_name}** to memory scratchpad!\\n\\n\" \\\n",
    "               f\"**Details:**\\n\" \\\n",
    "               f\"   API: {api_name}\\n\" \\\n",
    "               f\"   Source: {source_url}\\n\" \\\n",
    "               f\"   Endpoints: {doc_entry['endpoints_count']}\\n\" \\\n",
    "               f\"   Saved at: {doc_entry['created_at']}\\n\\n\" \\\n",
    "               f\"This documentation can now be reused for future API calls without re-extraction.\"\n",
    "               \n",
    "    except Exception as e:\n",
    "        return f\"Error writing to memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d954331",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_memory_documentation(api_name: str = None, website_url: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve a stored API documentation summary from the memory scratchpad.\n",
    "    This tool returns the full documentation entry for a given API name \n",
    "    or website URL, allowing reuse without re-extracting the source.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        memory = load_memory()\n",
    "        documentations = memory.get(\"documentations\", {})\n",
    "        \n",
    "        if not documentations:\n",
    "            return \"No documentation found in memory.\"\n",
    "        \n",
    "        # Find matching documentation\n",
    "        for doc_id, doc_data in documentations.items():\n",
    "            doc_url = doc_data.get(\"source_url\", \"\").lower()\n",
    "            doc_name = doc_data.get(\"api_name\", \"\").lower()\n",
    "            \n",
    "            if (api_name and api_name.lower() in doc_name) or \\\n",
    "               (website_url and website_url.lower() in doc_url):\n",
    "                \n",
    "                return f\"**Retrieved from Memory:** {doc_data['api_name']}\\n\" \\\n",
    "                       f\"Source: {doc_data['source_url']}\\n\" \\\n",
    "                       f\"Created: {doc_data['created_at']}\\n\\n\" \\\n",
    "                       f\"**Documentation Summary:**\\n{doc_data['summary']}\"\n",
    "        \n",
    "        return f\"No documentation found for {'API: ' + api_name if api_name else ''}{'Website: ' + website_url if website_url else ''}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving documentation from memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ac90e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools for Analysis Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "703dd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def download_kaggle_dataset(dataset_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Download a dataset from Kaggle and load it into memory.\n",
    "\n",
    "    This tool retrieves a dataset from Kaggle, searches for CSV files inside it, \n",
    "    and loads the first one into global memory for analysis. The dataset name \n",
    "    and DataFrame are stored globally, making them available for subsequent \n",
    "    operations without requiring another download.    \n",
    "    \"\"\"\n",
    "    global CURRENT_DATASET, CURRENT_DATASET_NAME\n",
    "    \n",
    "    try:\n",
    "        # Download dataset\n",
    "        temp_path = kagglehub.dataset_download(dataset_name)\n",
    "        csv_files = glob.glob(f\"{temp_path}/**/*.csv\", recursive=True)\n",
    "        \n",
    "        if not csv_files:\n",
    "            return \"No CSV files found in dataset.\"\n",
    "        \n",
    "        # Load the first CSV\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        \n",
    "        # Store globally\n",
    "        CURRENT_DATASET = df\n",
    "        CURRENT_DATASET_NAME = dataset_name.split('/')[-1]\n",
    "        \n",
    "        return f\"Dataset '{CURRENT_DATASET_NAME}' loaded! Shape: {df.shape}.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8109a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def analyze_data_with_pandas(analysis_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze the currently loaded dataset using a Pandas DataFrame agent.\n",
    "    This tool allows natural language queries on the dataset by leveraging \n",
    "    a Pandas agent that executes LLM-generated Python code under the hood. \n",
    "    It is mainly optimized for question answering tasks over tabular data.\n",
    "    \"\"\"\n",
    "    global CURRENT_DATASET, CURRENT_DATASET_NAME\n",
    "    \n",
    "    if CURRENT_DATASET is None:\n",
    "        return \"No dataset loaded. Please download a dataset first!\"\n",
    "    \n",
    "    try:\n",
    "        # Create pandas agent with current dataset\n",
    "        agent = create_pandas_dataframe_agent(\n",
    "            ChatOpenAI(temperature=0, model=\"gpt-4o-mini\"),\n",
    "            CURRENT_DATASET,\n",
    "            verbose=True,\n",
    "            agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "            allow_dangerous_code=True\n",
    "        )\n",
    "        \n",
    "        result = agent.invoke(analysis_query)\n",
    "        return f\"Analysis of {CURRENT_DATASET_NAME}:\\n\\n{result}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Analysis error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a50dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools Bindings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d980c657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "   1. extract_documentation_from_website: Extract documentation content from a given website URL\n",
      "   2. create_documentation_summary: Generate a concise summary of documentation content\n",
      "   3. fetch_api_data: Fetch data from a specified API endpoint\n",
      "   4. look_memory: Look up stored API documentation summaries in the memory scratchpad\n",
      "   5. write_memory: Save a new API documentation summary into the memory scratchpad\n",
      "   6. get_memory_documentation: Retrieve a stored API documentation summary from the memory scratchpad\n"
     ]
    }
   ],
   "source": [
    "# Tools Bindings for Context Agent\n",
    "tools_context_agent = [\n",
    "    extract_documentation_from_website, \n",
    "    create_documentation_summary,\n",
    "    fetch_api_data,\n",
    "    look_memory,\n",
    "    write_memory,\n",
    "    get_memory_documentation\n",
    "]\n",
    "\n",
    "print(\"Available tools:\")\n",
    "for i, tool in enumerate(tools_context_agent, 1):\n",
    "    print(f\"   {i}. {tool.name}: {tool.description.split('.')[0] if tool.description else 'No description'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6251f7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "   1. download_kaggle_dataset: Download a dataset from Kaggle and load it into memory\n",
      "   2. analyze_data_with_pandas: Analyze the currently loaded dataset using a Pandas DataFrame agent\n"
     ]
    }
   ],
   "source": [
    "# Tools Bindings for Analysis Agent\n",
    "tools_analysis_agent = [\n",
    "    download_kaggle_dataset,\n",
    "    analyze_data_with_pandas\n",
    "]\n",
    "\n",
    "print(\"Available tools:\")\n",
    "for i, tool in enumerate(tools_analysis_agent, 1):\n",
    "    print(f\"   {i}. {tool.name}: {tool.description.split('.')[0] if tool.description else 'No description'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4706d91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Systems Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f85d74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt Context Agent\n",
    "system_prompt_context_agent = \"\"\"\n",
    "You are a specialized API assistant with long-term memory capabilities that helps users learn and interact with APIs efficiently.\n",
    "\n",
    "## Your Memory System:\n",
    "You have access to a persistent memory scratchpad that stores API documentation summaries. This allows you to:\n",
    "- Avoid re-extracting documentation you've already processed\n",
    "- Provide faster responses by using cached knowledge\n",
    "- Build up a knowledge base of API documentation over time\n",
    "\n",
    "## Your Workflow:\n",
    "1. **ALWAYS start by checking memory** using `look_memory` to see if you already have documentation for the requested API\n",
    "2. **If documentation exists in memory**: Use `get_memory_documentation` to retrieve it and proceed directly to API calls\n",
    "3. **If no documentation in memory**: \n",
    "   - Extract documentation using `extract_documentation_from_website`\n",
    "   - Summarize it using `create_documentation_summary` \n",
    "   - Save it to memory using `write_memory`\n",
    "\n",
    "## Memory Management:\n",
    "- Use `look_memory(website_url=\"...\")` or `look_memory(api_name=\"...\")` to check for existing documentation\n",
    "- Use `write_memory(api_name=\"...\", source_url=\"...\", summary=\"...\")` to save new documentation\n",
    "- Use `get_memory_documentation(api_name=\"...\")` to retrieve full documentation from memory\n",
    "\n",
    "## Key Behaviors:\n",
    "- **Efficiency First**: Always check memory before extracting new documentation\n",
    "- **Memory Building**: Save all new documentation summaries to build your knowledge base\n",
    "- **Context Awareness**: Use the retrieved summary documentation to provide information on how to use the API\n",
    "- **User Communication**: Always explain what you're doing and whether you're using cached or new information\n",
    "- **Tool Usage**: You can call the tools as many times as you want to get the information you need\n",
    "\n",
    "## For OpenF1 API specifically:\n",
    "- Check memory for \"OpenF1\" or \"openf1.org\" documentation first\n",
    "- If not found, extract from https://openf1.org/#api-endpoints\n",
    "- Save the structured summary to memory for future use\n",
    "\n",
    "Be helpful, efficient, and always leverage your memory system to provide the best experience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "263aa1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt Analysis Agent\n",
    "system_prompt_analysis_agent = \"\"\"\n",
    "<PERSONA>\n",
    "You are an agent that can analyze a dataset using pandas. You download the dataset from Kaggle and analyze it with pandas.\n",
    "</PERSONA>\n",
    "\n",
    "<CONTEXT>\n",
    "The user can ask you to download a dataset from Kaggle and analyze it with pandas.\n",
    "\n",
    "download_kaggle_dataset: Use this tool to download a dataset from Kaggle and get information about it (shape, columns, sample data, and file path).\n",
    "analyze_data_with_pandas: Use this tool to analyze a dataset using pandas. THe user needs to provide de query to analyze the dataset.\n",
    "</CONTEXT>\n",
    "You can use the tools as many times as you want to get the information you need.\n",
    "Generate graphs and charts to help the user to understand the data, after generating the analysis you can use the same tool to generate the graph.\n",
    "\n",
    "Be helpful, efficient, and always leverage your memory system to provide the best experience.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99a519",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **ReAct Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6880a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Context Agent\n",
    "context_agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools_context_agent, \n",
    "    prompt=system_prompt_context_agent,\n",
    "    checkpointer=InMemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "235db684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Analysis Agent\n",
    "analysis_agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools_analysis_agent, \n",
    "    prompt=system_prompt_analysis_agent,\n",
    "    checkpointer=InMemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee047e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Testing Context Agent Behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print response of the Context Agent\n",
    "def test_context_agent(query: str, config: dict = None):\n",
    "    \"\"\"Test function with proper response handling\"\"\"\n",
    "    print(f\"[AGENT] Query: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = context_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "    }, config)\n",
    "    \n",
    "    print(\"Agent Response:\")\n",
    "    if isinstance(response, dict) and \"messages\" in response:\n",
    "        last_message = response[\"messages\"][-1]\n",
    "        if hasattr(last_message, 'content'):\n",
    "            print(last_message.content)\n",
    "        else:\n",
    "            print(str(last_message))\n",
    "    else:\n",
    "        if isinstance(response, list):\n",
    "            last_message = response[-1]\n",
    "            if hasattr(last_message, 'content'):\n",
    "                print(last_message.content)\n",
    "            else:\n",
    "                print(str(last_message))\n",
    "        else:\n",
    "            print(f\"Unexpected response format: {type(response)}\")\n",
    "            print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "619ac41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGENT] Query: Check what API documentation I have in memory\n",
      "--------------------------------------------------------------------------------\n",
      "Agent Response:\n",
      "It looks like there is currently no API documentation stored in memory. If you have a specific API in mind that you'd like to learn about or interact with, please let me know, and I can extract the documentation for you!\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check empty memory\n",
    "config = {\"configurable\": {\"thread_id\": \"test_session_1\"}}\n",
    "test_context_agent(\"Check what API documentation I have in memory\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9aff333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGENT] Query: Get information about the OpenF1 API. Please check if we already have documentation in memory, if not, extract from https://openf1.org/#api-endpoints\n",
      "--------------------------------------------------------------------------------\n",
      "Memory saved to memory_scratchpad_docs.json\n",
      "Agent Response:\n",
      "I have successfully extracted and saved the documentation for the OpenF1 API to memory. Here’s a brief overview of what it includes:\n",
      "\n",
      "### OpenF1 API Overview\n",
      "- **Description**: Provides real-time and historical Formula 1 data, including lap timings, car telemetry, and radio communications.\n",
      "- **Access**: Historical data is freely accessible; real-time data requires a paid account.\n",
      "- **Formats**: Data can be accessed in JSON or CSV formats.\n",
      "\n",
      "### Key Endpoints\n",
      "1. **Car Data**: Information about each car.\n",
      "2. **Drivers**: Information about drivers for each session.\n",
      "3. **Intervals**: Real-time interval data between drivers.\n",
      "4. **Laps**: Detailed information about individual laps.\n",
      "5. **Location**: Approximate location of cars on the circuit.\n",
      "6. **Meetings**: Information about Grand Prix or testing weekends.\n",
      "7. **Overtakes**: Information about overtakes during races.\n",
      "8. **Pit**: Information about cars going through the pit lane.\n",
      "9. **Position**: Driver positions throughout a session.\n",
      "10. **Race Control**: Information about race control incidents.\n",
      "11. **Sessions**: Information about sessions during a Grand Prix.\n",
      "12. **Weather**: Weather updates over the track.\n",
      "\n",
      "You can now ask me to interact with the OpenF1 API, and I can use this saved documentation to assist you efficiently! What would you like to do next?\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Learn OpenF1 API and save to memory\n",
    "test_context_agent(\"Get information about the OpenF1 API. Please check if we already have documentation in memory, if not, extract from https://openf1.org/#api-endpoints\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4555571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGENT] Query: \n",
      "Now I want to understand how to fetch information about the lap data from OpenF1 API. \n",
      "Please check memory first to see if we already have the documentation, \n",
      "and if so just explain how to use the API to get the lap data.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Agent Response:\n",
      "To fetch information about lap data from the OpenF1 API, you can use the following endpoint:\n",
      "\n",
      "### Laps Endpoint\n",
      "- **Endpoint**: `GET https://api.openf1.org/v1/laps`\n",
      "- **Description**: This endpoint provides detailed information about individual laps.\n",
      "\n",
      "### Required Attributes\n",
      "When making a request to this endpoint, you can include the following parameters to filter the results:\n",
      "- `session_key`: The unique identifier for the session (you can use `latest` to get the most recent session).\n",
      "- `driver_number`: The unique number assigned to an F1 driver.\n",
      "- `lap_number`: The sequential number of the lap within the session (starts at 1).\n",
      "\n",
      "### Example Request\n",
      "Here’s an example of how to fetch lap data for a specific driver and session:\n",
      "\n",
      "```bash\n",
      "curl \"https://api.openf1.org/v1/laps?session_key=9161&driver_number=63&lap_number=8\"\n",
      "```\n",
      "\n",
      "### Sample Response Attributes\n",
      "The response will include attributes such as:\n",
      "- `date_start`: The UTC starting date and time of the lap.\n",
      "- `driver_number`: The unique number assigned to the driver.\n",
      "- `duration_sector_1`, `duration_sector_2`, `duration_sector_3`: The time taken to complete each sector of the lap.\n",
      "- `lap_duration`: The total time taken to complete the lap.\n",
      "- `lap_number`: The sequential number of the lap.\n",
      "- `meeting_key`: The unique identifier for the meeting.\n",
      "- `session_key`: The unique identifier for the session.\n",
      "\n",
      "### Additional Notes\n",
      "- You can filter results by including additional parameters in the URL.\n",
      "- To receive results in CSV format, append `csv=true` to your URL.\n",
      "\n",
      "If you need help with a specific request or further details, feel free to ask!\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Verify memory was saved and test reuse\n",
    "test_context_agent(\"\"\"\n",
    "Now I want to understand how to fetch information about the lap data from OpenF1 API. \n",
    "Please check memory first to see if we already have the documentation, \n",
    "and if so just explain how to use the API to get the lap data.\n",
    "\"\"\", config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85912643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGENT] Query: Show me what API documentation summaries are stored in my memory scratchpad\n",
      "--------------------------------------------------------------------------------\n",
      "Agent Response:\n",
      "In your memory scratchpad, there is one API documentation summary stored:\n",
      "\n",
      "### OpenF1\n",
      "- **URL**: [OpenF1 API Documentation](https://openf1.org/#api-endpoints)\n",
      "- **Created**: September 17, 2025\n",
      "\n",
      "If you need to access this documentation or have any specific questions about it, just let me know!\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Check memory contents\n",
    "test_context_agent(\"Show me what API documentation summaries are stored in my memory scratchpad\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217389f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Test Analysis Agent Behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New session thread\n",
    "config = {\"configurable\": {\"thread_id\": \"test_session_2\"}}\n",
    "\n",
    "# Test 1: Download dataset from Kaggle\n",
    "response = analysis_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"rohanrao/formula-1-world-championship-1950-2020 download the dataset\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"Response:\", response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Perform basic analysis on the dataset\n",
    "response = analysis_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Perform some basic analysis on the dataset from Formula 1 World Championship 1950-2020 kaggle dataset you already have in memory the path\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"Response:\", response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Perform analysis\n",
    "response = analysis_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"perform some analysis about the altitude and the reference name of the circuit\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"Response:\", response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
