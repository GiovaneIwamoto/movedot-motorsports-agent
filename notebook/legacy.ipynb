{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53091614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain langchain_community langgraph langchain_openai\n",
    "%pip install --quiet langchain-experimental langgraph-supervisor\n",
    "%pip install --quiet tavily-python httpx pandas tabulate matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Keys\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tvly-dev-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "from tavily import TavilyClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba9629",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Set Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b948c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "llm_summarizer_agent = ChatOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], model=\"gpt-4.1\", temperature=0.1)\n",
    "llm_worker_agent = ChatOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c7f11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Set Utils for Scratchpad Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd68b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratchpad Memory Utils\n",
    "MEMORY_FILE = \"scratchpad_memory.json\"\n",
    "\n",
    "def initialize_memory_file():\n",
    "    \"\"\"Initialize the memory file if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(MEMORY_FILE):\n",
    "        initial_memory = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"last_updated\": datetime.now().isoformat(),\n",
    "            \"documentations\": {}\n",
    "        }\n",
    "        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(initial_memory, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Initialized memory file: {MEMORY_FILE}\")\n",
    "\n",
    "def load_memory() -> Dict[str, Any]:\n",
    "    \"\"\"Load the memory file\"\"\"\n",
    "    logger.info(f\"Loading memory from file: {MEMORY_FILE}\")\n",
    "    try:\n",
    "        with open(MEMORY_FILE, 'r', encoding='utf-8') as f:\n",
    "            memory_data = json.load(f)\n",
    "            logger.info(f\"Memory loaded successfully. Found {len(memory_data.get('documentations', {}))} documentation entries\")\n",
    "            return memory_data\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Memory file {MEMORY_FILE} not found, initializing...\")\n",
    "        initialize_memory_file()\n",
    "        return load_memory()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading memory: {e}\")\n",
    "        print(f\"Error loading memory: {e}\")\n",
    "        return {\"documentations\": {}}\n",
    "\n",
    "def save_memory(memory_data: Dict[str, Any]):\n",
    "    \"\"\"Save the memory file\"\"\"\n",
    "    logger.info(f\"Saving memory to file: {MEMORY_FILE}\")\n",
    "    try:\n",
    "        memory_data[\"last_updated\"] = datetime.now().isoformat()\n",
    "        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(memory_data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Memory saved successfully to {MEMORY_FILE}\")\n",
    "        print(f\"Memory saved to {MEMORY_FILE}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving memory: {e}\")\n",
    "        print(f\"Error saving memory: {e}\")\n",
    "\n",
    "# Initialize memory file\n",
    "initialize_memory_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67c98b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Set Utils for CSV Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Memory Utils\n",
    "CSV_MEMORY_FILE = \"csv_memory.json\"\n",
    "\n",
    "def initialize_csv_memory_file():\n",
    "    \"\"\"Initialize the CSV memory file if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(CSV_MEMORY_FILE):\n",
    "        initial_csv_memory = {\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"last_updated\": datetime.now().isoformat(),\n",
    "            \"csv_data\": {}\n",
    "        }\n",
    "        with open(CSV_MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(initial_csv_memory, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Initialized CSV memory file: {CSV_MEMORY_FILE}\")\n",
    "\n",
    "def load_csv_memory() -> Dict[str, Any]:\n",
    "    \"\"\"Load the CSV memory file\"\"\"\n",
    "    logger.info(f\"Loading CSV memory from file: {CSV_MEMORY_FILE}\")\n",
    "    try:\n",
    "        with open(CSV_MEMORY_FILE, 'r', encoding='utf-8') as f:\n",
    "            csv_memory_data = json.load(f)\n",
    "            logger.info(f\"CSV memory loaded successfully. Found {len(csv_memory_data.get('csv_data', {}))} CSV entries\")\n",
    "            return csv_memory_data\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"CSV memory file {CSV_MEMORY_FILE} not found, initializing...\")\n",
    "        initialize_csv_memory_file()\n",
    "        return load_csv_memory()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV memory: {e}\")\n",
    "        print(f\"Error loading CSV memory: {e}\")\n",
    "        return {\"csv_data\": {}}\n",
    "\n",
    "def save_csv_memory(csv_memory_data: Dict[str, Any]):\n",
    "    \"\"\"Save the CSV memory file\"\"\"\n",
    "    logger.info(f\"Saving CSV memory to file: {CSV_MEMORY_FILE}\")\n",
    "    try:\n",
    "        csv_memory_data[\"last_updated\"] = datetime.now().isoformat()\n",
    "        with open(CSV_MEMORY_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(csv_memory_data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"CSV memory saved successfully to {CSV_MEMORY_FILE}\")\n",
    "        print(f\"CSV memory saved to {CSV_MEMORY_FILE}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving CSV memory: {e}\")\n",
    "        print(f\"Error saving CSV memory: {e}\")\n",
    "\n",
    "def store_csv_data(csv_name: str, csv_content: str, source: str = \"OpenF1\"):\n",
    "    \"\"\"Store CSV data in persistent file\"\"\"\n",
    "    csv_memory = load_csv_memory()\n",
    "    csv_memory[\"csv_data\"][csv_name] = {\n",
    "        \"content\": csv_content,\n",
    "        \"source\": source,\n",
    "        \"stored_at\": datetime.now().isoformat(),\n",
    "        \"size\": len(csv_content)\n",
    "    }\n",
    "    save_csv_memory(csv_memory)\n",
    "    print(f\"CSV data stored: {csv_name} ({len(csv_content)} characters)\")\n",
    "\n",
    "def get_csv_data(csv_name: str) -> str:\n",
    "    \"\"\"Get CSV data from persistent file\"\"\"\n",
    "    csv_memory = load_csv_memory()\n",
    "    if csv_name in csv_memory.get(\"csv_data\", {}):\n",
    "        return csv_memory[\"csv_data\"][csv_name][\"content\"]\n",
    "    return None\n",
    "\n",
    "def list_available_csvs() -> Dict[str, Any]:\n",
    "    \"\"\"List all available CSV datasets in persistent storage\"\"\"\n",
    "    csv_memory = load_csv_memory()\n",
    "    csv_data = csv_memory.get(\"csv_data\", {})\n",
    "    \n",
    "    if not csv_data:\n",
    "        return {\"message\": \"No CSV datasets available\"}\n",
    "    \n",
    "    result = {\"available_datasets\": {}}\n",
    "    for name, data in csv_data.items():\n",
    "        result[\"available_datasets\"][name] = {\n",
    "            \"source\": data[\"source\"],\n",
    "            \"stored_at\": data[\"stored_at\"],\n",
    "            \"size\": data[\"size\"]\n",
    "        }\n",
    "    return result\n",
    "\n",
    "# Helper function for loading DataFrames\n",
    "def load_dataframe_from_csv(csv_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Load DataFrame from CSV data stored in persistent file\"\"\"\n",
    "    # Get CSV content from persistent storage\n",
    "    csv_content = get_csv_data(csv_name)\n",
    "    if csv_content is None:\n",
    "        raise ValueError(f\"CSV '{csv_name}' not found in persistent storage\")\n",
    "    \n",
    "    # Load DataFrame from CSV content\n",
    "    from io import StringIO\n",
    "    df = pd.read_csv(StringIO(csv_content))\n",
    "    \n",
    "    print(f\"DataFrame loaded: {csv_name} ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "    return df\n",
    "\n",
    "# Initialize CSV memory file\n",
    "initialize_csv_memory_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b40af2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools for Context Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94791263",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_documentation_from_website(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract documentation content from a given website URL.\n",
    "    This tool extracts raw documentation text from a website, \n",
    "    which can later be processed or analyzed to identify API endpoints, \n",
    "    parameters, and other technical details.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content_response = tavily_client.extract(urls=url)\n",
    "\n",
    "        if content_response and len(content_response) > 0:\n",
    "            return f\"Documentation extracted from {url}:\\n\\n{content_response}\"\n",
    "       \n",
    "        else:\n",
    "            return f\"Could not extract content from {url}. Please check the URL and try again.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error extracting documentation from {url}: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def extract_documentation_from_local_file(file_path: str = \"_api_endpoints.txt\") -> str:\n",
    "    \"\"\"\n",
    "    Load API endpoints documentation from a local file.\n",
    "    This tool reads the _api_endpoints.txt file directly, avoiding the need for web scraping.\n",
    "    The file contains comprehensive OpenF1 API documentation with all endpoints, parameters, and examples.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting extract_documentation_from_local_file with file_path: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists\n",
    "        logger.info(f\"Checking if file exists: {file_path}\")\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return f\"File {file_path} not found. Please ensure the file exists in the current directory.\"\n",
    "        \n",
    "        logger.info(f\"File exists, attempting to read content...\")\n",
    "        # Read the file content\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        logger.info(f\"File read successfully. Content length: {len(content)} characters\")\n",
    "        \n",
    "        if not content.strip():\n",
    "            logger.warning(f\"File {file_path} is empty\")\n",
    "            return f\"File {file_path} is empty.\"\n",
    "        \n",
    "        logger.info(f\"Returning documentation content successfully\")\n",
    "        return content\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading API endpoints from {file_path}: {str(e)}\")\n",
    "        return f\"Error loading API endpoints from {file_path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Debug Tools\n",
    "@tool\n",
    "def debug_csv_storage() -> str:\n",
    "    \"\"\"\n",
    "    Debug tool to check the current state of CSV storage.\n",
    "    This helps diagnose issues with data sharing between agents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = \"CSV STORAGE DEBUG:\\n\\n\"\n",
    "        \n",
    "        # Check persistent storage\n",
    "        csv_memory = load_csv_memory()\n",
    "        csv_data = csv_memory.get(\"csv_data\", {})\n",
    "        result += f\"Persistent CSV storage: {len(csv_data)} items\\n\"\n",
    "        for name, data in csv_data.items():\n",
    "            result += f\"  - {name}: {data['size']} chars, source: {data['source']}\\n\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error in debug_csv_storage: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def list_available_data() -> str:\n",
    "    \"\"\"\n",
    "    List all available data sources for analysis.\n",
    "    This provides a comprehensive view of what data is available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = \"AVAILABLE DATA SOURCES\\n\\n\"\n",
    "        \n",
    "        # Persistent Storage\n",
    "        result += \"CSV Storage:\\n\"\n",
    "        csv_memory = load_csv_memory()\n",
    "        csv_data = csv_memory.get(\"csv_data\", {})\n",
    "        if csv_data:\n",
    "            for name, data in csv_data.items():\n",
    "                result += f\"   - {name}: {data['size']} chars, source: {data['source']}\\n\"\n",
    "        else:\n",
    "            result += \"   - No CSV data available\\n\"\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error listing available data: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_documentation_summary(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a concise summary of documentation content.\n",
    "    This tool processes raw documentation text, removes irrelevant details, \n",
    "    and creates a well-formatted file with API endpoints and parameters.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting create_documentation_summary\")\n",
    "    \n",
    "    try:\n",
    "        system_prompt_summary_agent = \"\"\"\n",
    "        You are an expert in cleaning and restructuring API documentation.\n",
    "\n",
    "        You will receive raw documentation text. Your task is to transform it into a clear, structured, and developer-friendly reference.  \n",
    "        Keep all technical content intact and discard only irrelevant website elements such as navigation menus, headers, footers, ads, or unrelated links.\n",
    "\n",
    "        Always preserve and organize:\n",
    "        - Endpoint names, HTTP methods, and URLs\n",
    "        - Descriptions of what each endpoint does\n",
    "        - Parameters and filters, including their names, types, constraints, defaults, and full descriptions\n",
    "        - Response fields (attributes), with their names, meanings, units, and formats\n",
    "        - Request and response body structures\n",
    "        - Code snippets and sample URLs\n",
    "        - Authentication rules and error codes\n",
    "        - Notes, caveats, or important usage details\n",
    "\n",
    "        Do not shorten or drop technical details. Reformat them into a clean layout with clear sections (e.g. Overview, Parameters, Attributes, Examples, Notes). Use concise organization (headings, short lists, or tables) to improve readability, but keep all information provided by the source.\n",
    "\n",
    "        Your goal: produce a complete, accurate, and well-organized version of the documentation that is easy for developers to use, without losing any meaningful technical content.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"Creating prompt template...\")\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt_summary_agent),\n",
    "            (\"user\", \"Here is the content to be summarized: {content}\")\n",
    "        ])\n",
    "        \n",
    "        logger.info(\"Invoking LLM summarizer agent...\")\n",
    "        formatted_prompt = prompt.format(content=content)\n",
    "        logger.info(f\"Formatted prompt length: {len(formatted_prompt)}\")\n",
    "        \n",
    "        response = llm_summarizer_agent.invoke(formatted_prompt)\n",
    "        \n",
    "        logger.info(f\"LLM response received. Response length: {len(response.content)}\")\n",
    "        logger.info(f\"Response preview (first 200 chars): {response.content[:200]}...\")\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in create_documentation_summary: {str(e)}\")\n",
    "        return f\"Error creating documentation summary: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_api_data(endpoint: str, parameters: Dict[str, Any] = None) -> str:\n",
    "    \"\"\"\n",
    "    Fetch data from a specified API endpoint.\n",
    "    This tool constructs the request URL with optional parameters, \n",
    "    performs an HTTP GET request, and returns the response content.\n",
    "    For OpenF1 API endpoints, automatically requests CSV format and stores in memory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if this is an OpenF1 API endpoint\n",
    "        is_openf1 = \"api.openf1.org\" in endpoint\n",
    "        \n",
    "        if parameters:\n",
    "            # Convert parameters to query string\n",
    "            param_strings = []\n",
    "            for key, value in parameters.items():\n",
    "                param_strings.append(f\"{key}={value}\")\n",
    "            if param_strings:\n",
    "                endpoint += \"?\" + \"&\".join(param_strings)\n",
    "        \n",
    "        # For OpenF1 API, automatically append csv=true parameter (only if not already present)\n",
    "        if is_openf1 and \"csv=true\" not in endpoint:\n",
    "            separator = \"&\" if \"?\" in endpoint else \"?\"\n",
    "            endpoint += f\"{separator}csv=true\"\n",
    "        \n",
    "        # For OpenF1 API, check if CSV already exists before making request\n",
    "        if is_openf1:\n",
    "            csv_name = generate_csv_name(endpoint, parameters)\n",
    "            existing_csv = get_csv_data(csv_name)\n",
    "            if existing_csv:\n",
    "                return f\"CSV data already exists in memory as '{csv_name}' from {endpoint}\\n\\nData preview (first 5 lines):\\n{existing_csv.split(chr(10))[:5]}\\n\\nNo new API call needed - using cached data.\"\n",
    "        \n",
    "        # Make the HTTP request\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get(endpoint, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # For OpenF1 CSV responses, store in memory and return confirmation\n",
    "            if is_openf1 and response.headers.get('content-type', '').startswith('text/csv'):\n",
    "                # Generate CSV name based on endpoint (including all filters)\n",
    "                csv_name = generate_csv_name(endpoint, parameters)\n",
    "                store_csv_data(csv_name, response.text, \"OpenF1\")\n",
    "                return f\"CSV data fetched and stored as '{csv_name}' from {endpoint}\\n\\nData preview (first 5 lines):\\n{response.text.split(chr(10))[:5]}\"\n",
    "            \n",
    "            # For other APIs, try JSON first, then fall back to text\n",
    "            try:\n",
    "                data = response.json()\n",
    "                return f\"API Response from {endpoint}:\\n\\n{json.dumps(data, indent=2)}\"\n",
    "            except json.JSONDecodeError:\n",
    "                return f\"API Response from {endpoint}:\\n\\n{response.text}\"\n",
    "                \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        return f\"HTTP Error {e.response.status_code} when fetching {endpoint}: {e.response.text}\"\n",
    "    except httpx.TimeoutException:\n",
    "        return f\"Timeout error when fetching {endpoint}. The request took too long.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching data from {endpoint}: {str(e)}\"\n",
    "\n",
    "def generate_csv_name(endpoint: str, parameters: Dict[str, Any] = None) -> str:\n",
    "    \"\"\"Generate a CSV name based on endpoint and all parameters (including URL filters)\"\"\"\n",
    "    # Extract endpoint type (e.g., 'laps', 'sessions', 'drivers')\n",
    "    endpoint_clean = endpoint.split('?')[0].split('/')[-1] if endpoint else \"data\"\n",
    "    \n",
    "    # Extract all parameters from URL and combine with passed parameters\n",
    "    all_params = {}\n",
    "    \n",
    "    # Parse URL parameters\n",
    "    if '?' in endpoint:\n",
    "        url_params = endpoint.split('?')[1]\n",
    "        for param in url_params.split('&'):\n",
    "            if '=' in param:\n",
    "                key, value = param.split('=', 1)\n",
    "                if key != 'csv':  # Skip csv parameter\n",
    "                    all_params[key] = value\n",
    "    \n",
    "    # Add passed parameters (overriding URL params if same key)\n",
    "    if parameters:\n",
    "        for key, value in parameters.items():\n",
    "            if key != 'csv':  # Skip csv parameter\n",
    "                all_params[key] = value\n",
    "    \n",
    "    # Create suffix from all parameters\n",
    "    param_suffix = \"\"\n",
    "    if all_params:\n",
    "        param_parts = []\n",
    "        for key, value in sorted(all_params.items()):\n",
    "            # Clean parameter values for filename\n",
    "            clean_value = str(value).replace('=', '').replace('&', '').replace('?', '').replace('<', 'lt').replace('>', 'gt')\n",
    "            param_parts.append(f\"{key}_{clean_value}\")\n",
    "        if param_parts:\n",
    "            param_suffix = \"_\" + \"_\".join(param_parts)\n",
    "    \n",
    "    return f\"openf1_{endpoint_clean}{param_suffix}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def look_memory() -> str:\n",
    "    \"\"\"\n",
    "    Look up all stored API documentation summaries in the memory scratchpad.\n",
    "    This tool checks what documentation is already available in memory, \n",
    "    avoiding the need to re-extract and re-summarize.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting look_memory...\")\n",
    "    try:\n",
    "        memory = load_memory()\n",
    "        documentations = memory.get(\"documentations\", {})\n",
    "        \n",
    "        logger.info(f\"Found {len(documentations)} documentation entries in memory\")\n",
    "        \n",
    "        if not documentations:\n",
    "            logger.info(\"Memory is empty\")\n",
    "            return \"Memory scratchpad is empty. No API documentation summaries found.\"\n",
    "        \n",
    "        # Return overview of all documentation\n",
    "        result = f\"Memory scratchpad contains {len(documentations)} API documentation summary(ies):\\n\\n\"\n",
    "        for doc_id, doc_data in documentations.items():\n",
    "            result += f\"**{doc_data.get('api_name', 'Unknown API')}**\\n\"\n",
    "            result += f\"   Source: {doc_data.get('source_url', 'Unknown')}\\n\"\n",
    "            result += f\"   Created: {doc_data.get('created_at', 'Unknown')}\\n\"\n",
    "            result += f\"   Endpoints: {doc_data.get('endpoints_count', 0)}\\n\\n\"\n",
    "        \n",
    "        logger.info(\"Memory lookup completed successfully\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error looking up memory: {str(e)}\")\n",
    "        return f\"Error looking up memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def write_memory(api_name: str, summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Save a new API documentation summary into the memory scratchpad.\n",
    "    This tool stores structured documentation so it can be reused later \n",
    "    without re-extracting and re-summarizing the same API source.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting write_memory with api_name: {api_name}\")\n",
    "    logger.info(f\"Summary length: {len(summary)}\")\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Loading existing memory...\")\n",
    "        memory = load_memory()\n",
    "        logger.info(f\"Current memory has {len(memory.get('documentations', {}))} entries\")\n",
    "        \n",
    "        # Create a unique ID for this documentation\n",
    "        doc_id = f\"{api_name.lower().replace(' ', '_')}_{int(datetime.now().timestamp())}\"\n",
    "        logger.info(f\"Generated doc_id: {doc_id}\")\n",
    "        \n",
    "        # Try to parse endpoints count safely\n",
    "        endpoints_count = 0\n",
    "        try:\n",
    "            if summary.startswith('{'):\n",
    "                parsed_summary = json.loads(summary)\n",
    "                endpoints_count = len(parsed_summary.get(\"endpoints\", []))\n",
    "                logger.info(f\"Parsed JSON summary, found {endpoints_count} endpoints\")\n",
    "            else:\n",
    "                logger.info(\"Summary is not JSON format, setting endpoints_count to 0\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.warning(f\"Could not parse summary as JSON: {e}\")\n",
    "            endpoints_count = 0\n",
    "        \n",
    "        # Create the documentation entry\n",
    "        doc_entry = {\n",
    "            \"api_name\": api_name,\n",
    "            \"source_url\": \"local_file\",  # Default source for local files\n",
    "            \"summary\": summary,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"endpoints_count\": endpoints_count\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Created doc_entry: {doc_entry}\")\n",
    "        \n",
    "        # Add to memory\n",
    "        memory[\"documentations\"][doc_id] = doc_entry\n",
    "        logger.info(f\"Added entry to memory. Total entries now: {len(memory['documentations'])}\")\n",
    "        \n",
    "        # Save memory\n",
    "        logger.info(\"Saving memory to file...\")\n",
    "        save_memory(memory)\n",
    "        logger.info(\"Memory saved successfully\")\n",
    "        \n",
    "        return f\"Successfully saved documentation for **{api_name}** to memory scratchpad!\\n\\n\" \\\n",
    "               f\"**Details:**\\n\" \\\n",
    "               f\"   API: {api_name}\\n\" \\\n",
    "               f\"   Endpoints: {doc_entry['endpoints_count']}\\n\" \\\n",
    "               f\"   Saved at: {doc_entry['created_at']}\\n\\n\" \\\n",
    "               f\"This documentation can now be reused for future API calls without re-extraction.\"\n",
    "               \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to memory: {str(e)}\")\n",
    "        return f\"Error writing to memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e432299",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools for Analysis Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d954331",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_memory_documentation() -> str:\n",
    "    \"\"\"\n",
    "    Retrieve all stored API documentation summaries from the memory scratchpad.\n",
    "    This tool returns all documentation entries, allowing reuse without re-extracting the source.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting get_memory_documentation...\")\n",
    "    try:\n",
    "        memory = load_memory()\n",
    "        documentations = memory.get(\"documentations\", {})\n",
    "        \n",
    "        logger.info(f\"Found {len(documentations)} documentation entries\")\n",
    "        \n",
    "        if not documentations:\n",
    "            logger.info(\"No documentation found in memory\")\n",
    "            return \"No documentation found in memory.\"\n",
    "        \n",
    "        # Return all documentation\n",
    "        result = \"**Available Documentation in Memory:**\\n\\n\"\n",
    "        for doc_id, doc_data in documentations.items():\n",
    "            result += f\"**{doc_data['api_name']}**\\n\"\n",
    "            result += f\"Source: {doc_data['source_url']}\\n\"\n",
    "            result += f\"Created: {doc_data['created_at']}\\n\"\n",
    "            result += f\"Endpoints: {doc_data.get('endpoints_count', 0)}\\n\\n\"\n",
    "            result += f\"**Documentation Summary:**\\n{doc_data['summary']}\\n\\n\"\n",
    "            result += \"---\\n\\n\"\n",
    "        \n",
    "        logger.info(\"Documentation retrieval completed successfully\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving documentation from memory: {str(e)}\")\n",
    "        return f\"Error retrieving documentation from memory: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def analyze_data_with_pandas(analysis_query: str, csv_names: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Analyze CSV datasets using a Pandas DataFrame agent.\n",
    "    This tool loads CSV data from persistent storage and allows natural language queries \n",
    "    on the datasets by leveraging a Pandas agent that executes LLM-generated Python code.\n",
    "    It can work with multiple DataFrames simultaneously for comparative analysis.\n",
    "    \n",
    "    Args:\n",
    "        analysis_query: The analysis query in natural language\n",
    "        csv_names: Comma-separated list of CSV names to analyze. If None, analyzes all available CSVs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get list of available CSVs\n",
    "        available_csvs = list_available_csvs()\n",
    "        \n",
    "        if \"message\" in available_csvs:\n",
    "            return \"No CSV datasets available. Please fetch some data from OpenF1 API first.\"\n",
    "        \n",
    "        # Get available CSV names\n",
    "        available_names = list(available_csvs[\"available_datasets\"].keys())\n",
    "        \n",
    "        # Determine which CSVs to analyze\n",
    "        if csv_names:\n",
    "            csv_list = [name.strip() for name in csv_names.split(',')]\n",
    "            # Filter to only include available CSVs\n",
    "            csv_list = [name for name in csv_list if name in available_names]\n",
    "        else:\n",
    "            csv_list = available_names\n",
    "        \n",
    "        if not csv_list:\n",
    "            return \"No valid CSV names provided or no CSVs available.\"\n",
    "        \n",
    "        # Load DataFrames directly from persistent storage\n",
    "        dataframes_list = []\n",
    "        dataframe_names = []\n",
    "        \n",
    "        for csv_name in csv_list:\n",
    "            try:\n",
    "                # Load DataFrame using helper function\n",
    "                df = load_dataframe_from_csv(csv_name)\n",
    "                \n",
    "                # Create a clean name for the DataFrame\n",
    "                clean_name = csv_name.replace('.csv', '').replace('openf1_', '')\n",
    "                dataframes_list.append(df)\n",
    "                dataframe_names.append(f\"df_{clean_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {csv_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not dataframes_list:\n",
    "            return \"No DataFrames could be loaded successfully.\"\n",
    "        \n",
    "        # Create pandas agent with all dataframes\n",
    "        agent = create_pandas_dataframe_agent(\n",
    "            ChatOpenAI(temperature=0, model=\"gpt-4o-mini\"),\n",
    "            dataframes_list,\n",
    "            verbose=True,\n",
    "            agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "            allow_dangerous_code=True\n",
    "        )\n",
    "        \n",
    "        result = agent.invoke(analysis_query)\n",
    "        return f\"Analysis of {len(dataframes_list)} CSV datasets:\\n\\n{result}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Analysis error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a50dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Tools Bindings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools Bindings for Context Agent\n",
    "tools_context_agent = [\n",
    "    extract_documentation_from_website, \n",
    "    extract_documentation_from_local_file,\n",
    "    create_documentation_summary,\n",
    "    fetch_api_data,\n",
    "    look_memory,\n",
    "    write_memory,\n",
    "    get_memory_documentation,\n",
    "]\n",
    "\n",
    "print(\"Available tools:\")\n",
    "for i, tool in enumerate(tools_context_agent, 1):\n",
    "    print(f\"   {i}. {tool.name}: {tool.description.split('.')[0] if tool.description else 'No description'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools Bindings for Analysis Agent\n",
    "tools_analysis_agent = [\n",
    "    analyze_data_with_pandas,\n",
    "    debug_csv_storage,\n",
    "    list_available_data\n",
    "]\n",
    "\n",
    "print(\"Available tools:\")\n",
    "for i, tool in enumerate(tools_analysis_agent, 1):\n",
    "    print(f\"   {i}. {tool.name}: {tool.description.split('.')[0] if tool.description else 'No description'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4706d91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Systems Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt Context Agent\n",
    "system_prompt_context_agent = \"\"\"\n",
    "You are a specialized API assistant with long-term memory capabilities that helps users learn and interact with APIs efficiently.\n",
    "\n",
    "## Your Memory System:\n",
    "You have access to a persistent memory scratchpad that stores API documentation summaries. This allows you to:\n",
    "- Avoid re-extracting documentation you've already processed\n",
    "- Provide faster responses by using cached knowledge\n",
    "- Build up a knowledge base of API documentation over time\n",
    "\n",
    "## Your Workflow:\n",
    "1. **ALWAYS start by checking memory** using `look_memory` to see if you already have documentation for the requested API\n",
    "2. **If documentation exists in memory**: Use `get_memory_documentation` to retrieve it and proceed directly to API calls\n",
    "3. **If no documentation in memory**: \n",
    "   - **ALTERNATIVE**: Extract documentation using `extract_documentation_from_website`\n",
    "   - **MANUAL**: Summarize it using `create_documentation_summary` and save it to memory using `write_memory`\n",
    "\n",
    "## Available Tools and How to Use Them:\n",
    "\n",
    "### 1. Memory Management Tools:\n",
    "- `look_memory(website_url=\"https://example.com\")` - Check if documentation exists for a specific URL\n",
    "- `get_memory_documentation(api_name=\"example\")` - Retrieve full documentation from memory\n",
    "- `write_memory` - Save new documentation\n",
    "\n",
    "### 2. Documentation Tools (PREFERRED ORDER):\n",
    "- `load_api_endpoints_from_file(file_path=\"_api_endpoints.txt\")` - Load OpenF1 API documentation from local file\n",
    "- `extract_documentation_from_website(url=\"https://example.com\")` - Extract raw documentation from a website\n",
    "- `create_documentation_summary(content=\"...\")` - Convert raw documentation into structured summary\n",
    "\n",
    "### 3. API Data Fetching Tool:\n",
    "- `fetch_api_data(endpoint=\"https://api.example.com/v1/data\")` - Fetch data from any API endpoint\n",
    "   parameters:\n",
    "   - endpoint: The API endpoint to fetch data from\n",
    "   - parameters: A dictionary of parameters to pass to the API endpoint\n",
    "   IMPORTANT: Before fetching data, check if you have documentation for the API using `get_memory_documentation` and fetch the data using the documentation.\n",
    "   Never fetch data without documentation. Do not make API calls without documentation.\n",
    "\n",
    "**IMPORTANT**: For OpenF1 API, the system automatically adds `csv=true` parameter to get CSV format. You can still add other parameters:\n",
    "- Example: `fetch_api_data(endpoint=\"https://api.openf1.org/v1/laps\", parameters={\"example_parameter_1\": example_value_1, \"example_parameter_2\": example_value_2})`\n",
    "- This will become: `https://api.openf1.org/v1/laps?example_parameter_1=example_value_1&example_parameter_2=example_value_2&csv=true`\n",
    "\n",
    "## Key Behaviors:\n",
    "- **Efficiency First**: Always check memory before extracting new documentation\n",
    "- **Memory Building**: Save all new documentation summaries to build your knowledge base\n",
    "- **Context Awareness**: Use the retrieved summary documentation to provide information on how to use the API\n",
    "\n",
    "## CRITICAL: API Rate Limit Rules:\n",
    "- **AVOID making multiple API calls** when one comprehensive call would work\n",
    "- **ALWAYS fetch the largest dataset possible** in a single call\n",
    "- **Only apply specific filters** when user explicitly requests them\n",
    "- **STOP AFTER SUCCESSFUL FETCH**: Once data is successfully fetched and stored, provide the result and stop\n",
    "\n",
    "## Data Fetching Strategy - CRITICAL RULES:\n",
    "\n",
    "### Fetch only based on documentation. Do not make API calls without checking documentation first.\n",
    "\n",
    "### Fetching Examples:\n",
    "- **GOOD**: `fetch_api_data(endpoint=\"https://api.openf1.org/v1/laps\")` - Gets ALL laps\n",
    "- **GOOD**: `fetch_api_data(endpoint=\"https://api.openf1.org/v1/sessions\")` - Gets ALL sessions\n",
    "- **BAD**: Multiple calls like `fetch_api_data(..., parameters={\"driver_number\": 1})` then `fetch_api_data(..., parameters={\"driver_number\": 2})`\n",
    "- **BAD**: Fetching small filtered datasets when user asks for \"performance analysis\"\n",
    "\n",
    "### Smart Fetching Strategy:\n",
    "- **For general analysis requests**: Fetch complete datasets without filters\n",
    "- **For specific requests**: Only then apply the specific filter if it doesn't limit the analysis capabilities\n",
    "- **STOP AFTER SUCCESS**: Once data is fetched and stored, provide confirmation and stop\n",
    "\n",
    "## CRITICAL: STOP CONDITIONS:\n",
    "- **After successful data fetch**: Once you get a successful response and data is stored, STOP\n",
    "- **After providing documentation**: Once you provide API documentation information, STOP\n",
    "- **After answering user question**: Once you answer the user's question completely, STOP\n",
    "- **Do NOT make additional calls** unless the user specifically asks for more data or different parameters\n",
    "\n",
    "## WORKFLOW COMPLETION:\n",
    "When you complete your workflow, provide a clear summary of what you accomplished:\n",
    "- What documentation you found/summarized and saved to memory\n",
    "- What data you fetched and stored\n",
    "- What the user can do next\n",
    "\n",
    "Be helpful, efficient, and always leverage your memory system to provide the best experience. Remember: STOP after successful completion of each task and provide a clear summary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263aa1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt Analysis Agent\n",
    "system_prompt_analysis_agent = \"\"\"\n",
    "You are a specialized data analysis agent that can analyze CSV datasets using pandas and generate visualizations.\n",
    "\n",
    "## Your Context:\n",
    "You can analyze CSV data that was fetched by the Context Agent from APIs (like OpenF1). This data is stored persistently and can be accessed directly when needed. The system supports multiple DataFrames simultaneously, allowing for comparative analysis across different CSV files.\n",
    "\n",
    "## Available Tools and How to Use Them:\n",
    "\n",
    "### 1. Data Discovery Tools:\n",
    "- `list_available_data()` - Get a comprehensive view of all available data sources\n",
    "- `debug_csv_storage()` - Check what CSV data is available in persistent storage (more detailed)\n",
    "\n",
    "### 2. Analysis Tool:\n",
    "- `analyze_data_with_pandas(analysis_query=\"your question here\")` - Analyze all available CSV datasets\n",
    "- `analyze_data_with_pandas(analysis_query=\"your question here\", csv_names=\"dataset1,dataset2\")` - Analyze specific CSV datasets\n",
    "\n",
    "**IMPORTANT**: The analysis tool can:\n",
    "- Work with multiple DataFrames simultaneously\n",
    "- Perform joins and merges between DataFrames\n",
    "- Generate visualizations (graphs, charts, plots)\n",
    "- Execute complex pandas operations\n",
    "- Answer natural language questions about the data\n",
    "\n",
    "## Your Workflow - MANDATORY STEPS:\n",
    "1. **ALWAYS start by checking what data is available** using `list_available_data()` or `debug_csv_storage()`\n",
    "2. **If no data is available**: Ask the user to fetch data using the Context Agent first\n",
    "3. **BEFORE EVERY ANALYSIS**: Always call `list_available_data()` to get the most current list of datasets\n",
    "4. **Once data is available**: Use `analyze_data_with_pandas()` to perform analysis\n",
    "\n",
    "## CRITICAL: Always Check for New Data:\n",
    "- **NEVER assume** you know what datasets are available\n",
    "- **ALWAYS call** `list_available_data()` before any analysis\n",
    "- **New datasets** may have been added by the Context Agent since your last check\n",
    "- **This prevents** analyzing outdated or incomplete data\n",
    "\n",
    "## Analysis Capabilities:\n",
    "- **Multi-Dataset Analysis**: Compare data across different CSV files, perform joins, find relationships\n",
    "- **Visualization**: Generate graphs, charts, plots to help users understand the data\n",
    "- **Complex Queries**: Answer natural language questions about the data\n",
    "\n",
    "## Key Behaviors:\n",
    "- **Always check data availability first** before attempting analysis\n",
    "- **MANDATORY: Call `list_available_data()` before every analysis** to get current datasets\n",
    "- **Use natural language** for your analysis queries - the tool understands complex questions\n",
    "- **Generate visualizations** when they help explain the data\n",
    "- **Be specific** about which datasets to analyze when needed\n",
    "- **You can call tools multiple times** to get the information you need\n",
    "\n",
    "Be helpful, efficient, and always provide clear insights with visualizations when appropriate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99a519",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **ReAct Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6880a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Context Agent\n",
    "context_agent = create_react_agent(\n",
    "    model=llm_worker_agent, \n",
    "    tools=tools_context_agent, \n",
    "    prompt=system_prompt_context_agent,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    name=\"context_agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235db684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Analysis Agent\n",
    "analysis_agent = create_react_agent(\n",
    "    model=llm_worker_agent, \n",
    "    tools=tools_analysis_agent, \n",
    "    prompt=system_prompt_analysis_agent,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    name=\"analysis_agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3155d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Testing Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8406255",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"session_separeted_by_agent\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = context_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What documentation I have in memory?\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = analysis_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Which datasets I have in memory?\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = analysis_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Which datasets I have in memory?\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d32171",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = analysis_agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Debug the data in memory\"}]},\n",
    "    config\n",
    ")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfda2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Supervisor Agent**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph_supervisor import create_supervisor\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# # Create supervisor agent using langgraph-supervisor\n",
    "# pre_built_supervisor = create_supervisor(\n",
    "#     model=init_chat_model(\"openai:gpt-4o\"),\n",
    "#     agents=[context_agent, analysis_agent],\n",
    "#     prompt=(\n",
    "#         \"You are a supervisor managing two specialized agents:\\n\"\n",
    "#         \"- a context agent: Handles API documentation extraction, memory management, and data fetching from APIs like OpenF1\\n\"\n",
    "#         \"- an analysis agent: Performs data analysis on CSV datasets using pandas and generates visualizations\\n\"\n",
    "#         \"\\n\"\n",
    "#         \"CRITICAL RULES:\\n\"\n",
    "#         \"1. Assign work to one agent at a time, do not call agents in parallel\\n\"\n",
    "#         \"2. Do not do any work yourself - delegate all tasks to the appropriate agent\\n\"\n",
    "#         \"3. Let each agent complete their FULL workflow before considering the task done\\n\"\n",
    "#         \"4. For data-related workflows: first use the context agent to fetch data, then use the analysis agent to analyze it\\n\"\n",
    "#         \"5. WAIT for the agent to complete their entire internal workflow before making any decisions\\n\"\n",
    "#         \"6. Only delegate to another agent if the current agent explicitly indicates they cannot complete the task\\n\"\n",
    "#         \"7. Do not interrupt agents mid-workflow - let them finish their internal reasoning and tool calls\\n\"\n",
    "#         \"\\n\"\n",
    "#         \"The context agent has a specific workflow: check memory -> extract docs if needed -> fetch data -> store results\\n\"\n",
    "#         \"The analysis agent has a specific workflow: check available data -> perform analysis -> generate insights\\n\"\n",
    "#         \"Let each agent complete their full workflow before considering the task complete.\"\n",
    "#     ),\n",
    "#     add_handoff_back_messages=True,\n",
    "#     output_mode=\"full_history\",\n",
    "# ).compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c86ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Testing Pre-Built Supervisor Agent**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc645b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\"configurable\": {\"thread_id\": \"pre_built_supervisor_session\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the pre_built_supervisor with a multi-step task\n",
    "\n",
    "# response = pre_built_supervisor.invoke(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": \"What documentation do I have in memory?\"}]},\n",
    "#     config\n",
    "# )\n",
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the pre_built_supervisor with a multi-step task\n",
    "\n",
    "# response = pre_built_supervisor.invoke(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": \"Get the documentation from OpenF1 API and save it to memory?\"}]},\n",
    "#     config\n",
    "# )\n",
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the pre_built_supervisor with a multi-step task\n",
    "\n",
    "# response = pre_built_supervisor.invoke(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": \"Fetch and save to memory the data necessary to show me the most winners racers from Ferrari constructors\"}]},\n",
    "#     config\n",
    "# )\n",
    "# print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe32b9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Testing Custom Supervisor Agent**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.types import Command\n",
    "\n",
    "# Create handoff tool\n",
    "def create_handoff_tool(*, agent_name: str, description: str | None = None):\n",
    "    name = f\"transfer_to_{agent_name}\"\n",
    "    description = description or f\"Ask {agent_name} for help.\"\n",
    "\n",
    "    @tool(name, description=description)\n",
    "    def handoff_tool(\n",
    "        state: Annotated[MessagesState, InjectedState],\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    ) -> Command:\n",
    "        tool_message = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"Successfully transferred to {agent_name}\",\n",
    "            \"name\": name,\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(\n",
    "            goto=agent_name,  \n",
    "            update={**state, \"messages\": state[\"messages\"] + [tool_message]},  \n",
    "            graph=Command.PARENT,  \n",
    "        )\n",
    "    return handoff_tool\n",
    "\n",
    "\n",
    "# Handoffs\n",
    "assign_to_context_agent = create_handoff_tool(\n",
    "    agent_name=\"context_agent\",\n",
    "    description=\"Assign task to a context agent.\",\n",
    ")\n",
    "\n",
    "assign_to_analysis_agent = create_handoff_tool(\n",
    "    agent_name=\"analysis_agent\",\n",
    "    description=\"Assign task to a analysis agent.\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create supervisor agent with handoff tools\n",
    "supervisor_agent = create_react_agent(\n",
    "    model=ChatOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], model=\"gpt-4.1\"),\n",
    "    tools=[assign_to_context_agent, assign_to_analysis_agent],\n",
    "    prompt=(\n",
    "        \"You are a supervisor managing two specialized agents:\\n\"\n",
    "        \"- context_agent: Handles API documentation extraction, memory management, and data fetching\\n\"\n",
    "        \"- analysis_agent: Performs data analysis on CSV datasets using pandas\\n\"\n",
    "        \"\\n\"\n",
    "        \"Your job is to delegate tasks to the appropriate agent and let them complete their full workflow.\\n\"\n",
    "        \"Do not do any work yourself - delegate all tasks to the appropriate agent.\\n\"\n",
    "        \"Do not call agents in parallel - let them complete their full workflow.\\n\"\n",
    "        \"Do not interrupt them - let them finish their internal reasoning and tool calls.\\n\"\n",
    "        \"Only delegate to another agent if the current agent explicitly indicates they cannot complete the task.\\n\"\n",
    "        \"\\n\"\n",
    "        \"For data-related workflows: first use context_agent to fetch data, then use analysis_agent to analyze it.\"\n",
    "    ),\n",
    "    name=\"supervisor\"\n",
    ")\n",
    "\n",
    "# Create custom supervisor graph\n",
    "custom_supervisor = (\n",
    "    StateGraph(MessagesState)\n",
    "    .add_node(supervisor_agent, destinations={\"context_agent\", \"analysis_agent\", END})\n",
    "    .add_node(\"context_agent\", context_agent)\n",
    "    .add_node(\"analysis_agent\", analysis_agent)\n",
    "    .add_edge(START, \"supervisor\")\n",
    "    .add_edge(\"context_agent\", \"supervisor\")\n",
    "    .add_edge(\"analysis_agent\", \"supervisor\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31242050",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"custom_supervisor_session\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# Cannot get laps without filtering because OpenF1 exceeds the rate limit if do not filter.\n",
    "# Better models for supervisor and summarizer it influences how well the scratchpad is written."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
